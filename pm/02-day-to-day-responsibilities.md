# Day-to-Day Responsibilities of a Product Manager

You've got the theory. But what does a PM actually *do* all day? Let's get concrete.

---

## A Realistic Week in the Life

Here's what an actual Senior PM's week might look like at a mid-sized tech company.

This isn't aspirational—it's descriptive.

---

### Monday

- **9:00 AM** — Slack/email triage (30 min of "what's on fire?")
- **9:30 AM** — Weekly team standup
- **10:00 AM** — 1:1 with Engineering Manager
- **11:00 AM** — Customer call (enterprise prospect)
- **1:00 PM** — Sprint planning
- **3:00 PM** — Product review prep

---

### Tuesday

- **9:00 AM** — Design review for upcoming feature
- **10:00 AM** — Cross-team sync (Platform team dependency)
- **11:00 AM** — User research debrief with UXR
- **1:30 PM** — Deep work block (PRD writing, analysis)
- **4:00 PM** — 1:1 with APM you're mentoring

---

### Wednesday

- **9:30 AM** — Product team leads meeting
- **10:30 AM** — Engineering deep dive (technical design review)
- **12:00 PM** — Customer interview
- **1:00 PM** — Exec product review (present your area's progress)
- **3:30 PM** — Roadmap updates based on feedback

---

### What's Not on the Calendar

The unscheduled stuff eats a lot of time:

- Slack questions from engineering ("quick clarification on the spec")
- Urgent bugs or incidents requiring PM judgment
- Impromptu hallway conversations that turn into decisions
- The thing you forgot about until someone pinged you

---

> On average, PMs get about 2 hours of true "deep work" per day. Protect it fiercely.

---

## Discovery & Research

**What it is:** Understanding what problems exist and whether they're worth solving.

---

### Discovery Activities

- User interviews (1:1 conversations with real users)
- Survey design and analysis
- Usage data analysis (what are people actually doing?)
- Competitive research (what are others building?)
- Jobs-to-be-done interviews
- Support ticket and NPS analysis

---

### Discovery Mistakes

- Talking to users too late (after you've already decided)
- Asking leading questions ("Would you like it if we built X?")
- Confusing customer requests with customer problems
- Only talking to power users (survivorship bias)
- Letting data analysis replace qualitative understanding

---

### What Good Discovery Looks Like

- You have a continuous research cadence, not just "when we have time"
- You can articulate the top 5 user problems from memory
- Your roadmap priorities are clearly tied to research findings
- You're regularly surprised by what you learn (if not, you're confirming biases)

---

## Strategy & Roadmapping

**What it is:** Deciding what to build and when.

---

### The Roadmap Spectrum

Some companies want detailed 12-month roadmaps with dates. Others want high-level themes for the next quarter.

Know what your org expects.

---

### Low Fidelity (Themes)

- "Q1: Improve new user activation"
- "Q2: Expand enterprise capabilities"

**Good for:** Earlier stage, high uncertainty

---

### High Fidelity (Features with Dates)

- "Launch checkout redesign by March 15"
- "Complete API v2 migration by April 30"

**Good for:** Predictable work, stakeholder commitments

---

> **The trap:** Over-committing to dates on uncertain work. Your roadmap becomes a list of broken promises.

---

### What Good Roadmapping Looks Like

- Your roadmap clearly connects to company strategy
- You can explain why the top 3 priorities are the top 3
- Stakeholders know what to expect and trust the direction
- The roadmap evolves based on new information, not just top-down pressure

---

## Prioritization Frameworks

Prioritization is where strategy meets reality.

You'll have 50 things you could do and capacity for 5.

---

### RICE Scoring

**Reach × Impact × Confidence / Effort = RICE Score**

- **Reach:** How many users/customers affected?
- **Impact:** How much will it move the metric? (0.25 = minimal, 3 = massive)
- **Confidence:** How sure are you? (100% = high certainty, 50% = guess)
- **Effort:** Person-months to build

---

### RICE Scoring

**Good for:** Comparing opportunities with data, making prioritization legible

**Bad for:** Strategic bets (high uncertainty = low confidence = low scores)

---

### ICE Scoring

Simpler version: **Impact × Confidence × Ease**

All on 1-10 scales. Multiply them together.

Faster but less rigorous.

---

### The "Now / Next / Later" Framework

- **Now:** We're doing this immediately
- **Next:** High confidence this is coming
- **Later:** We want to do this eventually

Useful for communication, less useful for making the actual decisions.

---

> Experienced PMs use frameworks to *structure conversation*, not to make decisions for them.

If your spreadsheet says Feature A should be #1 but your gut says it's wrong—investigate that.

---

## Stakeholder Management

**Who are stakeholders?**

Anyone who cares about your product's success, has input into it, or is affected by it.

---

### Typical Stakeholders

- Engineering (your immediate team)
- Design (your immediate team)
- Leadership (execs who care about your area)
- Sales (they're selling your product)
- Marketing (they're promoting it)
- Support (they're fielding complaints)
- Legal/Compliance
- Other PM teams

---

### Stakeholder Management Basics

1. **Know what they care about** — What does success look like for *them*?
2. **Communicate appropriately** — Execs want headlines; engineers want details
3. **Set expectations early** — Surprises are the enemy
4. **Build trust through consistency** — Do what you say, say what you do

---

### Stakeholder Red Flags

- You're constantly surprised by stakeholder pushback
- Your launches are blocked at the last minute
- You find out important information late
- Other teams don't know what you're building

---

## Sprint & Development Collaboration

Once priorities are set, the work needs to actually get built.

---

### Your Role in Sprints

**Before sprint:** Ensure work is well-defined, dependencies identified, engineers have context

**During sprint:** Answer questions, unblock issues, manage scope creep

**End of sprint:** Accept/reject work, demo participation, retro participation

---

### The Spec → Build → Review Cycle

1. **Spec:** You write the requirements
2. **Refinement:** Engineering asks questions, estimates, identifies issues
3. **Design:** Designer creates the experience
4. **Build:** Engineers write code
5. **Review:** You verify it meets requirements
6. **Ship:** Deploy to users
7. **Measure:** Did it work?

---

### Common Collaboration Failures

- PM writes spec, throws it over the wall, disappears
- PM is too prescriptive about implementation
- PM is too vague ("make it better")
- PM changes requirements mid-sprint without acknowledging impact
- PM doesn't give engineers context on the "why"

---

### What Good Collaboration Looks Like

- Engineers feel ownership, not just assignment
- Questions get answered in hours, not days
- Scope changes are explicit and discussed
- Engineers would tell you if they think you're wrong

---

## Launch & Go-to-Market

The work doesn't end when the code is merged.

---

### Pre-Launch Checklist

- [ ] Feature is code-complete and QA'd
- [ ] Documentation updated
- [ ] Support team trained
- [ ] Marketing assets ready (if applicable)
- [ ] Sales team briefed (for B2B)
- [ ] Legal/compliance signed off
- [ ] Rollout plan defined
- [ ] Rollback plan defined
- [ ] Success metrics in place

---

### Launch Rituals Worth Having

- Launch readiness review (go/no-go meeting)
- Staged rollout (1% → 10% → 50% → 100%)
- War room for big launches
- Post-launch review

---

> **The difference between shipping and launching:**
>
> Shipping: Code goes live.
> Launching: Users know about it and adopt it.

Many PM failures happen in this gap.

---

## Metrics & Analysis

**You can't improve what you don't measure.**

---

### Metrics Hierarchy

1. **North Star Metric:** The one number that best captures value delivered (e.g., weekly active users, revenue)

2. **Input Metrics:** Leading indicators you can influence (e.g., signup conversion)

3. **Health Metrics:** Things you don't want to break (e.g., page load time, churn)

---

### Common Metrics by Function

| Function | Metrics |
|----------|---------|
| Acquisition | Signups, CAC, channel performance |
| Activation | Onboarding completion, time-to-value |
| Engagement | DAU/MAU, feature usage, session length |
| Retention | Churn rate, renewal rate, NPS |
| Revenue | ARPU, LTV, conversion rate |

---

### Analysis Rhythms

- **Daily:** Glance at key dashboards, spot anomalies
- **Weekly:** Review core metrics, compare to targets
- **Monthly:** Deeper analysis, trend identification
- **Quarterly:** Goal review, strategy adjustment

---

### Metrics Mistakes

- Vanity metrics (big number, no insight)
- Too many metrics (can't focus)
- Gaming metrics (hitting the number but missing the point)
- No baseline (up 20%—from what?)
- Correlation ≠ causation

---

## Documentation That Drives Outcomes

Writing is a PM's core tool.

---

## Product Requirements Document (PRD)

**Purpose:** Define what we're building and why.

---

### Good PRD Qualities

- Someone unfamiliar could understand the feature
- Clear "why" not just "what"
- Success is measurable
- Explicit about what's NOT included

---

## One-Pager

**Purpose:** Communicate a concept or proposal quickly.

**When to use:** Early-stage ideas, requesting resources, getting alignment.

**Structure:** Problem → Proposed solution → Why now → Success metrics → Key risks → Ask

Keep it to one page. Force yourself.

---

> Documentation quality correlates with product quality. Sloppy docs = sloppy thinking.

---

## Managing Up, Down, and Across

PM is influence without authority. Here's how it works directionally.

---

### Managing Up (Leadership)

**What leadership wants:**
- Confidence that your area is handled
- No surprises
- Clear asks when you need something
- Progress on things they care about

---

### Managing Up (Tactics)

- Regular updates (proactive, not just when asked)
- Crisp communication (headline, then detail if needed)
- Recommendations with rationale (not just problems)
- Escalate early if things are off-track

**Cardinal sin:** Surprises.

---

### Managing Across (Peers)

- Understand their goals and pressures
- Help them succeed (it comes back around)
- Be reliable in shared commitments
- Give credit generously

---

### Handling Peer Conflict

- Assume good intent first
- Name the disagreement directly
- Focus on interests, not positions
- Escalate together if stuck (not around each other)

---

## Common Time Traps

Where PM time goes to die.

---

### The Reactive Spiral

Living in Slack, responding to everything immediately.

Feels productive; isn't.

**Fix:** Time-box communication. Check Slack 3x daily, not 300x.

---

### Death by Meetings

Your calendar is full, but you accomplish nothing.

**Fix:** Audit ruthlessly. Block deep work time. Say no.

---

### Over-Documenting

100-page PRDs that no one reads.

**Fix:** Shorter documents, more conversations. Iterate on docs.

---

### Shiny Object Syndrome

New idea every week. Nothing gets finished.

**Fix:** Commit to priorities. Complete before switching. Write down ideas for later.

---

### Analysis Paralysis

Waiting for perfect data before deciding.

**Fix:** Two-way door decisions should be faster. Accept uncertainty. You can adjust.

---

> Your day-to-day should serve your strategy. If you can't connect how you spent today to what matters for your product—something's off.

---

Now let's look at how AI is changing all of this.
